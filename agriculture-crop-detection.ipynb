{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2011301,"sourceType":"datasetVersion","datasetId":775339},{"sourceId":193,"sourceType":"modelInstanceVersion","modelInstanceId":137}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"raw","source":"import numpy as np \nimport pandas as pd \n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-11T15:04:02.150733Z","iopub.execute_input":"2024-02-11T15:04:02.151094Z","iopub.status.idle":"2024-02-11T15:04:02.203088Z","shell.execute_reply.started":"2024-02-11T15:04:02.151064Z","shell.execute_reply":"2024-02-11T15:04:02.202022Z"}}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\nimport IPython\nfrom IPython.display import display, Image\nfrom PIL import Image","metadata":{"execution":{"iopub.status.busy":"2024-02-11T15:04:02.204727Z","iopub.execute_input":"2024-02-11T15:04:02.205010Z","iopub.status.idle":"2024-02-11T15:04:02.210655Z","shell.execute_reply.started":"2024-02-11T15:04:02.204986Z","shell.execute_reply":"2024-02-11T15:04:02.209654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import Image, display\n\ndirectory = \"../input/agriculture-crop-images/crop_images/jute/jute0\"\ndirectoryAlt = \"../input/agriculture-crop-images/crop_images/jute/jute00\"\nfile_extension = \".jpeg\"\n\nfor file_num in range(1, 41):\n    if file_num < 10:\n        filenameAlt = directoryAlt + str(file_num)+ 'a' + file_extension\n        display(Image(filename=filenameAlt)) \n    else:\n        filename = directory + str(file_num)+ 'a' + file_extension\n        display(Image(filename=filename))\n        \nprint(\"----------------------------------------------------------------------------\\n\")\nprint(\"____________________________________________________________________________\\n\")\n\ndirectory = \"../input/agriculture-crop-images/crop_images/maize/maize0\"\ndirectoryAlt = \"../input/agriculture-crop-images/crop_images/maize/maize00\"\nfile_extension = \".jpeg\"\n\nfor file_num in range(1, 41):\n    if file_num < 10 and not(file_num == 6 or file_num == 7):\n        filenameAlt = directoryAlt + str(file_num)+ 'a' + file_extension\n        display(Image(filename=filenameAlt)) \n    elif file_num == 6 or file_num == 7:\n        filenameAlt = directoryAlt + str(file_num)+ file_extension\n        display(Image(filename=filenameAlt))\n    elif file_num == 14:\n        filenameAlt = directory + str(file_num)+ file_extension\n        display(Image(filename=filenameAlt))\n    else:\n        filename = directory + str(file_num)+ 'a' + file_extension\n        display(Image(filename=filename))\n\nprint(\"----------------------------------------------------------------------------\\n\")\nprint(\"____________________________________________________________________________\\n\")\n\ndirectory = \"../input/agriculture-crop-images/crop_images/rice/rice0\"\ndirectoryAlt = \"../input/agriculture-crop-images/crop_images/rice/rice00\"\nfile_extension = \".jpeg\"\n\nfor file_num in range(1, 41):\n    if file_num < 10:\n        filenameAlt = directoryAlt + str(file_num)+ 'a' + file_extension\n        display(Image(filename=filenameAlt)) \n    else:\n        filename = directory + str(file_num)+ 'a' + file_extension\n        display(Image(filename=filename))\n        \nprint(\"----------------------------------------------------------------------------\\n\")\nprint(\"____________________________________________________________________________\\n\")\n\ndirectory = \"../input/agriculture-crop-images/crop_images/sugarcane/sugarcane000\"\ndirectoryAlt = \"../input/agriculture-crop-images/crop_images/sugarcane/sugarcane00\"\ndirectoryAlts = \"../input/agriculture-crop-images/crop_images/sugarcane/sugarcane0\"\nfile_extension = \".jpeg\"\n\nfor file_num in range(1, 41):\n    if file_num <= 4:\n        filenameAlt = directory + str(file_num)+ 'a' + file_extension\n        display(Image(filename=filenameAlt)) \n    elif file_num>=5 and file_num<=9:\n        filenameAlt = directoryAlt + str(file_num)+ 'a' + file_extension\n        display(Image(filename=filenameAlt)) \n    elif file_num==10 or file_num==11:\n        filenameAlt = directoryAlt + str(file_num)+ 'a' + file_extension\n        display(Image(filename=filenameAlt)) \n    else:\n        filename = directoryAlts + str(file_num)+ 'a' + file_extension\n        display(Image(filename=filename))\n\nprint(\"----------------------------------------------------------------------------\\n\")\nprint(\"____________________________________________________________________________\\n\")\n\ndirectory = \"../input/agriculture-crop-images/crop_images/sugarcane/sugarcane000\"\ndirectoryAlt = \"../input/agriculture-crop-images/crop_images/sugarcane/sugarcane00\"\ndirectoryAlts = \"../input/agriculture-crop-images/crop_images/sugarcane/sugarcane0\"\nfile_extension = \".jpeg\"\n\nfor file_num in range(1, 41):\n    if file_num <= 4:\n        filenameAlt = directory + str(file_num)+ 'a' + file_extension\n        display(Image(filename=filenameAlt)) \n    elif file_num>=5 and file_num<=9:\n        filenameAlt = directoryAlt + str(file_num)+ 'a' + file_extension\n        display(Image(filename=filenameAlt)) \n    elif file_num==10 or file_num==11:\n        filenameAlt = directoryAlt + str(file_num)+ 'a' + file_extension\n        display(Image(filename=filenameAlt)) \n    else:\n        filename = directoryAlts + str(file_num)+ 'a' + file_extension\n        display(Image(filename=filename))\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-11T15:04:02.212100Z","iopub.execute_input":"2024-02-11T15:04:02.212667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport os\ninput_folder = \"/kaggle/input/agriculture-crop-images/test_crop_image\"\noutput_folder = \"/kaggle/working\"\ntarget_size = 224\n\nfor filename in os.listdir(input_folder):\n    if filename.endswith(('.jpg', '.jpeg', '.png', '.bmp')):\n        image_path = os.path.join(input_folder, filename)\n        img = cv2.imread(image_path)\n        resized_img = cv2.resize(img, (target_size, target_size))\n        output_path = os.path.join(output_folder, filename)\n        cv2.imwrite(output_path, resized_img)\n        print(f\"Resized {filename} successfully.\")","metadata":{"execution":{"iopub.status.busy":"2024-02-11T15:08:28.164663Z","iopub.execute_input":"2024-02-11T15:08:28.165365Z","iopub.status.idle":"2024-02-11T15:08:28.784974Z","shell.execute_reply.started":"2024-02-11T15:08:28.165333Z","shell.execute_reply":"2024-02-11T15:08:28.784025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\nfilename = \"../input/agriculture-crop-images/crop_images/jute/jute001a.jpeg\"\nimage = Image.open(filename)\nwidth, height = image.size\n\nprint(f\"width is: {width} height is: {height}\")\n#poggers\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import display, Image\n\nfilenameAlt = \"/kaggle/working/sugarcane-field.jpg\"\ndisplay(Image(filename=filenameAlt))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.applications.resnet50 import preprocess_input\nfrom tensorflow.keras.preprocessing import image\n\n# Load a pre-trained CNN model (e.g., ResNet50)\nbase_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n\n# Function to preprocess and extract features from an image\ndef extract_features(image_path, model):\n    img = image.load_img(image_path, target_size=(224, 224))\n    img_array = image.img_to_array(img)\n    img_array = np.expand_dims(img_array, axis=0)\n    img_array = preprocess_input(img_array)\n    features = model.predict(img_array)\n    return features.flatten()\n\n# Example image path\nimage_path = 'example.jpg'\n\n# Extract features from the image\nimage_features = extract_features(image_path, base_model)\n\n# Now `image_features` contains the vector representation of the image\nprint(image_features.shape)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport os\nPATH = os.path.join(\"/kaggle/input/agriculture-crop-images\")\n\ntrain_dir = os.path.join(PATH, 'kag2')\nvalidation_dir = os.path.join(PATH, '/kaggle/working/')\n\nBATCH_SIZE = 32\nIMG_SIZE = (224, 224)\n\ntrain_dataset = tf.keras.utils.image_dataset_from_directory(train_dir,\n                                                            shuffle=True,\n                                                            batch_size=BATCH_SIZE,\n                                                            image_size=IMG_SIZE)\nvalidation_dataset = tf.keras.utils.image_dataset_from_directory(train_dir,\n                                                                shuffle=True,\n                                                                batch_size=BATCH_SIZE,\n                                                                image_size=IMG_SIZE)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-11T15:08:35.591420Z","iopub.execute_input":"2024-02-11T15:08:35.591800Z","iopub.status.idle":"2024-02-11T15:08:35.752922Z","shell.execute_reply.started":"2024-02-11T15:08:35.591756Z","shell.execute_reply":"2024-02-11T15:08:35.752167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nclass_names = train_dataset.class_names\n\nplt.figure(figsize=(10, 10))\nfor images, labels in train_dataset.take(1):\n  for i in range(16):\n    ax = plt.subplot(4, 4, i + 1)\n    plt.imshow(images[i].numpy().astype(\"uint8\"))\n    plt.title(class_names[labels[i]])\n    plt.axis(\"off\")","metadata":{"execution":{"iopub.status.busy":"2024-02-11T15:08:37.013148Z","iopub.execute_input":"2024-02-11T15:08:37.013520Z","iopub.status.idle":"2024-02-11T15:08:38.769878Z","shell.execute_reply.started":"2024-02-11T15:08:37.013490Z","shell.execute_reply":"2024-02-11T15:08:38.768584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"AUTOTUNE = tf.data.AUTOTUNE\n\ntrain_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\nvalidation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2024-02-11T15:08:43.800660Z","iopub.execute_input":"2024-02-11T15:08:43.801059Z","iopub.status.idle":"2024-02-11T15:08:43.808520Z","shell.execute_reply.started":"2024-02-11T15:08:43.801033Z","shell.execute_reply":"2024-02-11T15:08:43.807487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_augmentation = tf.keras.Sequential([\n  tf.keras.layers.RandomFlip('horizontal'),\n  tf.keras.layers.RandomRotation(0.2),\n])","metadata":{"execution":{"iopub.status.busy":"2024-02-11T15:08:41.856232Z","iopub.execute_input":"2024-02-11T15:08:41.856600Z","iopub.status.idle":"2024-02-11T15:08:41.867307Z","shell.execute_reply.started":"2024-02-11T15:08:41.856570Z","shell.execute_reply":"2024-02-11T15:08:41.866271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for image, _ in train_dataset.take(1):\n  plt.figure(figsize=(10, 10))\n  first_image = image[0]\n  for i in range(9):\n    ax = plt.subplot(3, 3, i + 1)\n    augmented_image = data_augmentation(tf.expand_dims(first_image, 0))\n    plt.imshow(augmented_image[0] / 255)\n    plt.axis('off')","metadata":{"execution":{"iopub.status.busy":"2024-02-11T15:08:45.803635Z","iopub.execute_input":"2024-02-11T15:08:45.804276Z","iopub.status.idle":"2024-02-11T15:08:47.041739Z","shell.execute_reply.started":"2024-02-11T15:08:45.804242Z","shell.execute_reply":"2024-02-11T15:08:47.040790Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preprocess_input = tf.keras.applications.efficientnet_v2.preprocess_input\nrescale = tf.keras.layers.Rescaling(1./127.5, offset=-1)","metadata":{"execution":{"iopub.status.busy":"2024-02-11T15:08:49.390058Z","iopub.execute_input":"2024-02-11T15:08:49.390441Z","iopub.status.idle":"2024-02-11T15:08:49.396452Z","shell.execute_reply.started":"2024-02-11T15:08:49.390410Z","shell.execute_reply":"2024-02-11T15:08:49.395353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow_hub as hub\nIMG_SHAPE = IMG_SIZE + (3,)\nbase_model = tf.keras.applications.EfficientNetV2L(input_shape=IMG_SHAPE,\n                                               include_top=False,\n                                               weights='imagenet')\n#base_model = hub.KerasLayer(\"https://www.kaggle.com/models/google/efficientnet-v2/frameworks/TensorFlow2/variations/imagenet1k-b0-classification/versions/2\")","metadata":{"execution":{"iopub.status.busy":"2024-02-11T15:08:50.673818Z","iopub.execute_input":"2024-02-11T15:08:50.674683Z","iopub.status.idle":"2024-02-11T15:09:44.094640Z","shell.execute_reply.started":"2024-02-11T15:08:50.674648Z","shell.execute_reply":"2024-02-11T15:09:44.093585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_batch, label_batch = next(iter(train_dataset))\nfeature_batch = base_model(image_batch)\nprint(feature_batch.shape)","metadata":{"execution":{"iopub.status.busy":"2024-02-11T15:10:39.497149Z","iopub.execute_input":"2024-02-11T15:10:39.497545Z","iopub.status.idle":"2024-02-11T15:10:40.376552Z","shell.execute_reply.started":"2024-02-11T15:10:39.497512Z","shell.execute_reply":"2024-02-11T15:10:40.375387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_model.trainable = False\nloss = []","metadata":{"execution":{"iopub.status.busy":"2024-02-11T15:10:41.593676Z","iopub.execute_input":"2024-02-11T15:10:41.594090Z","iopub.status.idle":"2024-02-11T15:10:41.637065Z","shell.execute_reply.started":"2024-02-11T15:10:41.594058Z","shell.execute_reply":"2024-02-11T15:10:41.636169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\nfeature_batch_average = global_average_layer(feature_batch)\nprint(feature_batch_average.shape)","metadata":{"execution":{"iopub.status.busy":"2024-02-11T15:10:43.329741Z","iopub.execute_input":"2024-02-11T15:10:43.330430Z","iopub.status.idle":"2024-02-11T15:10:43.337649Z","shell.execute_reply.started":"2024-02-11T15:10:43.330396Z","shell.execute_reply":"2024-02-11T15:10:43.336697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction_layer = tf.keras.layers.Dense(1)\nprediction_batch = prediction_layer(feature_batch_average)\nprint(prediction_batch.shape)","metadata":{"execution":{"iopub.status.busy":"2024-02-11T15:10:44.942384Z","iopub.execute_input":"2024-02-11T15:10:44.942979Z","iopub.status.idle":"2024-02-11T15:10:44.958894Z","shell.execute_reply.started":"2024-02-11T15:10:44.942943Z","shell.execute_reply":"2024-02-11T15:10:44.957959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"inputs = tf.keras.Input(shape=(224, 224, 3))\nx = data_augmentation(inputs)\nx = preprocess_input(x)\nx = base_model(x, training=False)\nx = global_average_layer(x)\nx = tf.keras.layers.Dropout(0.2)(x)\noutputs = prediction_layer(x)\nmodel = tf.keras.Model(inputs, outputs)","metadata":{"execution":{"iopub.status.busy":"2024-02-11T15:10:47.555499Z","iopub.execute_input":"2024-02-11T15:10:47.555900Z","iopub.status.idle":"2024-02-11T15:10:51.183958Z","shell.execute_reply.started":"2024-02-11T15:10:47.555869Z","shell.execute_reply":"2024-02-11T15:10:51.183135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_learning_rate = 0.0001\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics=[tf.keras.metrics.BinaryAccuracy(threshold=0, name='accuracy')])","metadata":{"execution":{"iopub.status.busy":"2024-02-11T15:20:31.236166Z","iopub.status.idle":"2024-02-11T15:20:31.236494Z","shell.execute_reply.started":"2024-02-11T15:20:31.236333Z","shell.execute_reply":"2024-02-11T15:20:31.236348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"initial_epochs = 100\n#change epochs to 100\ncheckpoint_filepath = '/kaggle/working'\n\nmodel.fit(train_dataset,epochs=initial_epochs)","metadata":{"execution":{"iopub.status.busy":"2024-02-11T15:20:31.234577Z","iopub.status.idle":"2024-02-11T15:20:31.234989Z","shell.execute_reply.started":"2024-02-11T15:20:31.234799Z","shell.execute_reply":"2024-02-11T15:20:31.234822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"m = model.load_weights('/kaggle/working/saved_model.pb')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"initial_epochs = 100\n\nloss0, accuracy0 = model.evaluate(validation_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-02-11T15:20:31.237380Z","iopub.status.idle":"2024-02-11T15:20:31.237718Z","shell.execute_reply.started":"2024-02-11T15:20:31.237543Z","shell.execute_reply":"2024-02-11T15:20:31.237557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"initial loss: {:.2f}\".format(loss0))\nprint(\"initial accuracy: {:.2f}\".format(accuracy0))\nhistory = model.fit(train_dataset,\n                    epochs=initial_epochs,\n                    validation_data=validation_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-02-11T15:24:59.234139Z","iopub.execute_input":"2024-02-11T15:24:59.235002Z","iopub.status.idle":"2024-02-11T15:25:23.133394Z","shell.execute_reply.started":"2024-02-11T15:24:59.234965Z","shell.execute_reply":"2024-02-11T15:25:23.131733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-02-11T15:26:47.856686Z","iopub.execute_input":"2024-02-11T15:26:47.857569Z","iopub.status.idle":"2024-02-11T15:26:47.864303Z","shell.execute_reply.started":"2024-02-11T15:26:47.857533Z","shell.execute_reply":"2024-02-11T15:26:47.863298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}